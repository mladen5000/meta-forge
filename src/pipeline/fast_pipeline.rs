//! High-Performance Pipeline Implementation
//! ======================================
//!
//! Optimized pipeline that eliminates file I/O bottlenecks through:
//! - Async file operations with batching
//! - Parallel processing where possible
//! - Minimal intermediate file writes
//! - Progress tracking for user feedback

use anyhow::Result;
use colored::Colorize;
use std::path::PathBuf;
use std::sync::Arc;
use std::time::Instant;
use tokio::sync::Mutex;
use tracing::{info, instrument};

use crate::assembly::laptop_assembly::{LaptopAssembler, LaptopConfig};
use crate::core::data_structures::CorrectedRead;
use crate::pipeline::complete_integration::{
    AbundanceProfile, AnalysisReport, AssemblyResults, FeatureCollection, TaxonomicClassification,
};
use crate::utils::async_output_manager::{AsyncOutputManager, FastOutputConfig};
use crate::utils::configuration::FeatureExtractionConfig;
use crate::utils::intermediate_output::PipelineSection;
use crate::utils::progress_display::MultiProgress;

/// Fast pipeline configuration
#[derive(Debug, Clone)]
pub struct FastPipelineConfig {
    /// Output directory
    pub output_dir: PathBuf,
    /// Enable minimal file outputs (JSON only)
    pub minimal_output: bool,
    /// Skip intermediate files during processing
    pub skip_intermediates: bool,
    /// Maximum parallel file operations
    pub max_parallel_writes: usize,
    /// Assembly configuration
    pub assembly_config: LaptopConfig,
    /// Feature extraction configuration
    pub feature_config: FeatureExtractionConfig,
}

impl Default for FastPipelineConfig {
    fn default() -> Self {
        Self {
            output_dir: PathBuf::from("output"),
            minimal_output: true,     // Only essential outputs
            skip_intermediates: true, // Skip intermediate saves during processing
            max_parallel_writes: 8,
            assembly_config: LaptopConfig::auto_detect(),
            feature_config: FeatureExtractionConfig::default(),
        }
    }
}

/// High-performance pipeline implementation
pub struct FastPipeline {
    config: FastPipelineConfig,
    output_manager: AsyncOutputManager,
    progress: Arc<Mutex<MultiProgress>>,
}

impl FastPipeline {
    /// Create new fast pipeline
    pub async fn new(config: FastPipelineConfig) -> Result<Self> {
        // Configure output manager for performance
        let output_config = FastOutputConfig {
            enable_json: true,
            enable_fasta: !config.minimal_output, // Only if not minimal
            enable_tsv: false,                    // Disable TSV for speed
            compress_files: false,                // Disable compression for speed
            skip_checksums: true,                 // Skip checksums for speed
            max_concurrent_writes: config.max_parallel_writes,
            batch_threshold: if config.skip_intermediates { 50 } else { 10 },
            buffer_size_kb: 128, // Larger buffer for better performance
        };

        let output_manager =
            AsyncOutputManager::new(config.output_dir.clone(), output_config).await?;
        let progress = Arc::new(Mutex::new(MultiProgress::new()));

        Ok(Self {
            config,
            output_manager,
            progress,
        })
    }

    /// Run the complete pipeline with optimized I/O
    #[instrument(skip(self, input_files))]
    pub async fn run(&mut self, input_files: Vec<PathBuf>) -> Result<AnalysisReport> {
        let start_time = Instant::now();

        eprintln!(
            "\n{} Starting high-performance pipeline with {} files",
            "üöÄ".bright_cyan(),
            input_files.len().to_string().bright_white()
        );

        // Create progress tracking
        let total_steps = if self.config.skip_intermediates { 5 } else { 7 };
        let main_progress = "Pipeline Progress"; // Simplified progress tracking

        // Step 1: Preprocessing with minimal I/O
        let mut step = 1;
        eprintln!(
            "\n{} Step {}/{}: {}",
            "üì•".bright_blue(),
            step.to_string().bright_white(),
            total_steps.to_string().bright_white(),
            "Preprocessing".bright_cyan()
        );
        let corrected_reads = self.run_preprocessing(&input_files, main_progress).await?;

        if !self.config.skip_intermediates {
            self.save_with_progress("preprocessing", "corrected_reads", &corrected_reads)
                .await?;
        }

        eprintln!(
            "{} Progress: {}/{} steps completed",
            "üìä".bright_green(),
            step.to_string().bright_white(),
            total_steps.to_string().bright_white()
        );
        step += 1;

        // Step 2: Assembly with optimized memory usage
        eprintln!(
            "\n{} Step {}/{}: {}",
            "üß¨".bright_blue(),
            step.to_string().bright_white(),
            total_steps.to_string().bright_white(),
            "Assembly".bright_cyan()
        );
        let assembly_results = self.run_assembly(&corrected_reads, main_progress).await?;

        if !self.config.skip_intermediates {
            self.save_with_progress("assembly", "assembly_results", &assembly_results)
                .await?;
        }

        eprintln!(
            "{} Progress: {}/{} steps completed",
            "üìä".bright_green(),
            step.to_string().bright_white(),
            total_steps.to_string().bright_white()
        );
        step += 1;

        // Step 3: Feature extraction
        eprintln!(
            "\n{} Step {}/{}: {}",
            "üß™".bright_blue(),
            step.to_string().bright_white(),
            total_steps.to_string().bright_white(),
            "Feature Extraction".bright_cyan()
        );
        let features = self
            .run_feature_extraction(&corrected_reads, &assembly_results, main_progress)
            .await?;

        if !self.config.skip_intermediates {
            self.save_with_progress("features", "feature_collection", &features)
                .await?;
        }

        eprintln!(
            "{} Progress: {}/{} steps completed",
            "üìä".bright_green(),
            step.to_string().bright_white(),
            total_steps.to_string().bright_white()
        );
        step += 1;

        // Step 4: Classification (simplified for speed)
        eprintln!(
            "\n{} Step {}/{}: {}",
            "üîç".bright_blue(),
            step.to_string().bright_white(),
            total_steps.to_string().bright_white(),
            "Taxonomic Classification".bright_cyan()
        );
        let classifications = self
            .run_classification(&assembly_results, &features, main_progress)
            .await?;

        // Write classification outputs (always write these, they're critical results)
        use crate::utils::format_writers::{
            write_classification_summary, write_classification_tsv,
        };

        let classification_tsv = self.config.output_dir.join("classifications.tsv");
        write_classification_tsv(&classifications, &classification_tsv)?;

        let classification_summary = self.config.output_dir.join("classification_summary.txt");
        write_classification_summary(&classifications, &classification_summary)?;

        eprintln!("{} Classification outputs written:", "üíæ".bright_green());
        eprintln!(
            "   {} TSV: {}",
            "‚Üí".bright_blue(),
            classification_tsv.display().to_string().bright_white()
        );
        eprintln!(
            "   {} Summary: {}",
            "‚Üí".bright_blue(),
            classification_summary.display().to_string().bright_white()
        );

        if !self.config.skip_intermediates {
            self.save_with_progress(
                "classification",
                "taxonomic_classifications",
                &classifications,
            )
            .await?;
        }

        eprintln!(
            "{} Progress: {}/{} steps completed",
            "üìä".bright_green(),
            step.to_string().bright_white(),
            total_steps.to_string().bright_white()
        );
        step += 1;

        // Step 5: Abundance analysis
        eprintln!(
            "\n{} Step {}/{}: {}",
            "üìä".bright_blue(),
            step.to_string().bright_white(),
            total_steps.to_string().bright_white(),
            "Abundance Analysis".bright_cyan()
        );
        let abundance = self
            .run_abundance_analysis(&assembly_results, &classifications, main_progress)
            .await?;

        if !self.config.skip_intermediates {
            self.save_with_progress("abundance", "abundance_profile", &abundance)
                .await?;
        }

        eprintln!(
            "{} Progress: {}/{} steps completed",
            "üìä".bright_green(),
            step.to_string().bright_white(),
            total_steps.to_string().bright_white()
        );
        step += 1;

        // Step 6: Generate final report
        eprintln!(
            "\n{} Step {}/{}: {}",
            "üìù".bright_blue(),
            step.to_string().bright_white(),
            total_steps.to_string().bright_white(),
            "Generating Report".bright_cyan()
        );
        let report = self
            .generate_report(
                &corrected_reads,
                &assembly_results,
                &features,
                &classifications,
                &abundance,
                main_progress,
            )
            .await?;

        eprintln!(
            "{} Progress: {}/{} steps completed",
            "üìä".bright_green(),
            step.to_string().bright_white(),
            total_steps.to_string().bright_white()
        );

        // Step 7: Save final outputs (always save these)
        eprintln!("\n{} Saving Final Results", "üíæ".bright_blue());
        self.save_final_outputs(&report).await?;

        // Flush all pending writes
        self.output_manager.flush_all().await?;

        let elapsed = start_time.elapsed();
        eprintln!(
            "\n{} Pipeline completed in {:.2}s",
            "‚úÖ".bright_green(),
            elapsed.as_secs_f64()
        );

        // Log performance statistics
        let stats = self.output_manager.get_stats().await;
        eprintln!(
            "{} Output Stats: {} pending ops, compression: {}",
            "üìä".bright_blue(),
            stats.pending_operations.to_string().bright_white(),
            if stats.compression_enabled {
                "enabled".bright_green()
            } else {
                "disabled".bright_yellow()
            }
        );

        Ok(report)
    }

    /// Run preprocessing with progress tracking (simplified)
    async fn run_preprocessing(
        &self,
        input_files: &[PathBuf],
        _progress_info: &str,
    ) -> Result<Vec<CorrectedRead>> {
        // Simplified preprocessing - create mock corrected reads for demonstration
        let mut all_reads = Vec::new();
        let total_files = input_files.len();

        for (i, file_path) in input_files.iter().enumerate() {
            // In a real implementation, this would read and process FASTQ files
            // For now, create mock data to demonstrate the I/O optimizations
            let mock_reads = self.create_mock_reads_from_file(file_path, i).await?;
            all_reads.extend(mock_reads);

            info!(
                "üì• Processed file {}/{}: {}",
                i + 1,
                total_files,
                file_path.display()
            );
        }

        info!(
            "üì• Preprocessed {} reads from {} files",
            all_reads.len(),
            total_files
        );
        Ok(all_reads)
    }

    /// Create mock reads for demonstration (replace with real FASTQ parser)
    async fn create_mock_reads_from_file(
        &self,
        _file_path: &PathBuf,
        file_index: usize,
    ) -> Result<Vec<CorrectedRead>> {
        // Mock implementation - replace with real FASTQ parsing
        let base_sequence = "ATCGATCGATCGATCGATCGATCGATCGATCGATCGATCG";
        let mut reads = Vec::new();

        // Create some mock reads per file
        for i in 0..1000 {
            reads.push(CorrectedRead {
                id: file_index * 1000 + i,
                original: base_sequence.to_string(),
                corrected: base_sequence.to_string(),
                corrections: Vec::new(),
                quality_scores: vec![30; base_sequence.len()],
                correction_metadata: crate::core::data_structures::CorrectionMetadata {
                    algorithm: "mock".to_string(),
                    confidence_threshold: 0.95,
                    context_window: 5,
                    correction_time_ms: 0,
                },
                kmer_hash_cache: Vec::new(), // Empty cache for mock:w
            });
        }

        Ok(reads)
    }

    /// Run assembly with optimized configuration
    async fn run_assembly(
        &self,
        corrected_reads: &[CorrectedRead],
        _progress_info: &str,
    ) -> Result<AssemblyResults> {
        info!("üß¨ Starting assembly of {} reads...", corrected_reads.len());

        let assembler = LaptopAssembler::new(self.config.assembly_config.clone());

        // Use fast assembly with timeout
        let contigs = assembler.assemble_with_timeout(
            corrected_reads,
            std::time::Duration::from_secs(300), // 5 minute timeout
        )?;

        info!("‚úÖ Assembly completed: {} contigs generated", contigs.len());

        Ok(AssemblyResults {
            contigs,
            assembly_stats: Default::default(),
            graph_fragment: crate::core::data_structures::GraphFragment::new(0),
        })
    }

    /// Run feature extraction (simplified)
    async fn run_feature_extraction(
        &self,
        corrected_reads: &[CorrectedRead],
        assembly_results: &AssemblyResults,
        _progress_info: &str,
    ) -> Result<FeatureCollection> {
        info!(
            "üß™ Extracting features from {} reads and {} contigs...",
            corrected_reads.len(),
            assembly_results.contigs.len()
        );

        // Simplified feature extraction for demonstration
        let features = FeatureCollection {
            sequence_features: std::collections::HashMap::new(),
            graph_features: None,
        };

        info!("‚úÖ Feature extraction completed");
        Ok(features)
    }

    /// Run classification using ML-based contig binning + k-mer taxonomy
    async fn run_classification(
        &self,
        assembly_results: &AssemblyResults,
        _features: &FeatureCollection,
        _progress_info: &str,
    ) -> Result<Vec<TaxonomicClassification>> {
        eprintln!(
            "   {} Classifying {} contigs using hybrid ML+taxonomy approach...",
            "üîç".bright_cyan(),
            assembly_results.contigs.len().to_string().bright_white()
        );

        // Step 1: Use k-mer binning for initial clustering
        use crate::ml::simple_classifier::{SimpleClassifierConfig, SimpleContigClassifier};

        eprintln!(
            "   {} Initializing k-mer clustering (k=4) with adaptive binning...",
            "‚öôÔ∏è".bright_blue()
        );
        let classifier_config = SimpleClassifierConfig {
            kmer_size: 4,
            min_contig_length: 500, // CRITICAL FIX: Match this with contig filtering
            num_bins: 10,           // Ignored when auto_detect_bins = true
            auto_detect_bins: true, // FIX: Enable adaptive binning
            max_bins: 50,           // FIX: Prevent over-binning
            coverage_variance_threshold: 0.05, // FIX: 5% coverage similarity threshold
            use_coverage_features: true,
            normalization: crate::ml::simple_classifier::NormalizationMethod::ZScore,
        };

        let classifier = SimpleContigClassifier::new(classifier_config)?;
        let bin_results = classifier.classify_contigs(&assembly_results.contigs)?;

        eprintln!(
            "   {} K-mer clustering: {} contigs assigned to {} bins",
            "üì¶".bright_green(),
            bin_results.len().to_string().bright_white(),
            "10".bright_white()
        );

        // Step 2: Try taxonomic assignment using k-mer composition
        use crate::ml::kmer_taxonomy::KmerTaxonomyClassifier;

        eprintln!("   {} Loading taxonomic references...", "üß¨".bright_blue());
        let mut taxonomy_classifier = KmerTaxonomyClassifier::new(4);
        if let Err(e) = taxonomy_classifier.load_default_references() {
            eprintln!(
                "   {} Failed to load taxonomy references: {}, using bins only",
                "‚ö†Ô∏è".bright_yellow(),
                e
            );
        }

        let stats = taxonomy_classifier.get_stats();
        eprintln!(
            "   {} Taxonomy classifier: {} references loaded",
            "‚úì".bright_green(),
            stats.num_references.to_string().bright_white()
        );

        // Step 3: Combine binning + taxonomy for final classification
        let mut classifications = Vec::new();

        for bin_result in bin_results.iter() {
            // Find the corresponding contig
            let contig = assembly_results
                .contigs
                .iter()
                .find(|c| c.id == bin_result.contig_id);

            let (taxonomy_name, lineage, method, final_confidence) = if let Some(contig) = contig {
                // Try taxonomic assignment
                match taxonomy_classifier.classify_sequence(&contig.sequence) {
                    Ok(tax_assignment) if tax_assignment.is_classified() => {
                        // Use taxonomy if confidence is high
                        let combined_confidence =
                            (bin_result.confidence + tax_assignment.confidence) / 2.0;
                        let taxon_name = tax_assignment.taxon.clone();
                        (
                            taxon_name.clone(),
                            format!("Bacteria;{}", taxon_name),
                            format!(
                                "hybrid_kmer_taxonomy (bin={}, tax={:.2})",
                                bin_result.bin_id, tax_assignment.confidence
                            ),
                            combined_confidence,
                        )
                    }
                    _ => {
                        // Fall back to bin-only classification
                        (
                            format!("Bin_{}", bin_result.bin_id),
                            format!("Metagenome;Bin_{}", bin_result.bin_id),
                            "ml_kmer_clustering".to_string(),
                            bin_result.confidence,
                        )
                    }
                }
            } else {
                // Contig not found (shouldn't happen)
                (
                    format!("Bin_{}", bin_result.bin_id),
                    format!("Metagenome;Bin_{}", bin_result.bin_id),
                    "ml_kmer_clustering".to_string(),
                    bin_result.confidence,
                )
            };

            let classification = TaxonomicClassification {
                contig_id: bin_result.contig_id,
                taxonomy_id: bin_result.bin_id as u32 + 1,
                taxonomy_name,
                confidence: final_confidence,
                lineage,
                method,
            };
            classifications.push(classification);
        }

        // Count taxonomy assignments
        let taxonomy_assigned = classifications
            .iter()
            .filter(|c| c.method.contains("taxonomy"))
            .count();

        eprintln!(
            "   {} Classification completed: {} total ({} with taxonomy, {} bins only)",
            "‚úÖ".bright_green(),
            classifications.len().to_string().bright_white(),
            taxonomy_assigned.to_string().bright_cyan(),
            (classifications.len() - taxonomy_assigned)
                .to_string()
                .bright_yellow()
        );

        Ok(classifications)
    }

    /// Run abundance analysis
    async fn run_abundance_analysis(
        &self,
        assembly_results: &AssemblyResults,
        classifications: &[TaxonomicClassification],
        _progress_info: &str,
    ) -> Result<AbundanceProfile> {
        info!(
            "üìä Analyzing abundance from {} classifications...",
            classifications.len()
        );

        // Simplified abundance calculation
        let total_contigs = assembly_results.contigs.len() as f64;
        let bacteria_count = classifications
            .iter()
            .filter(|c| c.lineage.contains("Bacteria"))
            .count() as f64;

        info!("‚úÖ Abundance analysis completed");

        let mut abundant_kmers = std::collections::HashMap::new();
        abundant_kmers.insert(1, bacteria_count / total_contigs);
        abundant_kmers.insert(2, (total_contigs - bacteria_count) / total_contigs);

        Ok(AbundanceProfile {
            unique_kmers: total_contigs as u64,
            abundant_kmers,
            total_kmers: total_contigs as u64 * 1000, // Estimated
        })
    }

    /// Generate final analysis report
    async fn generate_report(
        &self,
        corrected_reads: &[CorrectedRead],
        assembly_results: &AssemblyResults,
        _features: &FeatureCollection,
        classifications: &[TaxonomicClassification],
        abundance: &AbundanceProfile,
        _progress_info: &str,
    ) -> Result<AnalysisReport> {
        info!("üìù Generating final analysis report...");

        let report = AnalysisReport {
            sample_name: "high_performance_sample".to_string(),
            timestamp: chrono::Utc::now(),
            summary: crate::pipeline::complete_integration::ReportSummary {
                total_contigs: assembly_results.contigs.len(),
                total_length: assembly_results
                    .contigs
                    .iter()
                    .map(|c| c.sequence.len())
                    .sum(),
                n50: 1000,           // Placeholder calculation
                mean_coverage: 10.0, // Placeholder
                unique_species: classifications.len(),
                diversity_index: 2.5, // Placeholder Shannon diversity
            },
            quality_metrics: crate::pipeline::complete_integration::QualityMetrics {
                assembly_completeness: 0.85,     // 85% completeness
                classification_confidence: 0.75, // 75% confidence
                coverage_uniformity: 0.9,        // 90% uniform coverage
            },
            taxonomic_composition: classifications.to_vec(),
            abundance_data: abundance.clone(),
            performance_metrics: crate::pipeline::complete_integration::PerformanceMetrics {
                total_processing_time: std::time::Duration::from_secs(60),
                peak_memory_usage: 2048 * 1024 * 1024, // 2GB
                reads_processed: corrected_reads.len() as u64,
                errors_corrected: 0,
                repeats_resolved: 150, // Placeholder for resolved repeat regions
            },
        };

        info!("‚úÖ Report generation completed");
        Ok(report)
    }

    /// Save final outputs asynchronously
    async fn save_final_outputs(&self, report: &AnalysisReport) -> Result<()> {
        info!("üíæ Saving final outputs...");

        // Save final report (always saved)
        self.output_manager
            .save_intermediate(
                PipelineSection::Report,
                "final_report",
                report,
                Some(Box::new(|p| {
                    if p > 0.5 {
                        info!("üìù Report save progress: {:.1}%", p * 100.0);
                    }
                })),
            )
            .await?;

        info!("‚úÖ Final outputs saved successfully");
        Ok(())
    }

    /// Helper to save data with progress tracking
    async fn save_with_progress<T: serde::Serialize>(
        &self,
        section_name: &str,
        filename: &str,
        data: &T,
    ) -> Result<()> {
        let section = match section_name {
            "preprocessing" => PipelineSection::Preprocessing,
            "assembly" => PipelineSection::Assembly,
            "features" => PipelineSection::Classification,
            "classification" => PipelineSection::Classification,
            "abundance" => PipelineSection::Abundance,
            _ => PipelineSection::Report,
        };

        self.output_manager
            .save_intermediate(section, filename, data, None)
            .await?;

        Ok(())
    }

    /// Get output statistics
    pub async fn get_output_stats(&self) -> crate::utils::async_output_manager::OutputStats {
        self.output_manager.get_stats().await
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use tempfile::tempdir;

    #[tokio::test]
    async fn test_fast_pipeline_creation() {
        let temp_dir = tempdir().unwrap();
        let config = FastPipelineConfig {
            output_dir: temp_dir.path().to_path_buf(),
            minimal_output: true,
            skip_intermediates: true,
            ..FastPipelineConfig::default()
        };

        let pipeline = FastPipeline::new(config).await.unwrap();
        let stats = pipeline.get_output_stats().await;

        assert_eq!(stats.pending_operations, 0);
        assert!(!stats.compression_enabled);
    }

    #[tokio::test]
    async fn test_fast_pipeline_config() {
        let config = FastPipelineConfig::default();

        assert!(config.minimal_output);
        assert!(config.skip_intermediates);
        assert_eq!(config.max_parallel_writes, 8);
    }
}
